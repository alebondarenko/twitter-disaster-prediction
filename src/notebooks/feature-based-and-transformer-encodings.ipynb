{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, make_scorer, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def strip_punct(s):\n",
    "    s = re.sub('[^A-Za-z0-9]', ' ', s)\n",
    "    s = s.lower()\n",
    "    return \" \".join(s.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls ../../data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1       0        0  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4       0        0             Forest fire near La Ronge Sask. Canada   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('../../data/train.csv')\n",
    "train_data = train_data.fillna(0)\n",
    "train_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 3271, 0: 4342})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text = np.array(train_data.text.apply(lambda x: strip_punct(x)).tolist())\n",
    "y_train = np.array(train_data.target.tolist())\n",
    "Counter(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR feature based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,1), analyzer='word', min_df = 1, token_pattern='(?u)\\\\b\\\\w+\\\\b')\n",
    "vectorizer.fit(train_text)\n",
    "X_train = vectorizer.transform(train_text)\n",
    "\n",
    "tuned_parameters = {'penalty' : ['l1', 'l2'],\n",
    "                    'C' : np.logspace(-4, 4, 20),\n",
    "                    'solver' : ['liblinear', 'lbfgs']}\n",
    "\n",
    "lr = LogisticRegression()\n",
    "clf = GridSearchCV(lr, tuned_parameters, cv=5, scoring='f1')\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Best parameters set found on the train set:\")\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=5)\n",
    "y_trainCv, y_predCv =  list(), list()\n",
    "\n",
    "for train_index, test_index in tqdm(kf.split(train_text, y_train)):\n",
    "    tr_text, ts_text = train_text[train_index], train_text[test_index]\n",
    "    y_tr, y_ts = y_train[train_index], y_train[test_index]\n",
    "    y_trainCv.extend(y_ts)\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1,1), analyzer='word', min_df = 1, token_pattern='(?u)\\\\b\\\\w+\\\\b')\n",
    "    vectorizer.fit(tr_text)\n",
    "    X_tr = vectorizer.transform(tr_text)\n",
    "    X_ts = vectorizer.transform(ts_text)\n",
    "\n",
    "    clf = LogisticRegression(C=1, penalty='l2', solver='liblinear')\n",
    "#    clf = SVC()\n",
    "    clf.fit(X_tr, y_tr)\n",
    "    predictions = clf.predict(X_ts)\n",
    "    y_predCv.extend(predictions)\n",
    "\n",
    "print(classification_report(y_trainCv, y_predCv, digits=3, zero_division=False))\n",
    "print('F1: {:.3f}'.format(f1_score(y_trainCv, y_predCv, zero_division=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#max len of the tweets in tokens\n",
    "lens = list()\n",
    "for s in train_text:\n",
    "    lens.append(len(s.split()))\n",
    "max(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = np.array(train_data.text.tolist()) # Try without cleaning\n",
    "#train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "from transformers import pipeline\n",
    "\n",
    "feature_extraction = pipeline('feature-extraction', model=\"bert-base-cased\", tokenizer=\"bert-base-cased\", device=-1) # device=-1 for CPU, device=0 for GPU\n",
    "X_train = list()\n",
    "for sentence in tqdm(train_text):\n",
    "    features = feature_extraction(sentence)\n",
    "    X_train.append(features[0][0])\n",
    "\n",
    "#X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "935aa9cde42944668ff14e883a2add39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1425941629.0, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c259dbb9e0884e979b6599f38fd1e32d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=898823.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae396cede54243a8bf50baf0c735eb63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "578eb93f3fb348caaeafbd426a6e1347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1355863.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7613/7613 [23:26<00:00,  5.41it/s]\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "from transformers import pipeline\n",
    "\n",
    "feature_extraction = pipeline('feature-extraction', model=\"roberta-large\", tokenizer=\"roberta-large\", device=-1) # device=-1 for CPU, device=0 for GPU\n",
    "X_train = list()\n",
    "for sentence in tqdm(train_text):\n",
    "    features = feature_extraction(sentence)\n",
    "    X_train.append(features[0][0])\n",
    "\n",
    "#X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tuned_parameters = {'penalty' : ['l1', 'l2'],\n",
    "                    'C' : np.logspace(-4, 4, 20),\n",
    "                    'solver' : ['liblinear', 'lbfgs']}\n",
    "\n",
    "lr = LogisticRegression()\n",
    "clf = GridSearchCV(lr, tuned_parameters, cv=5, scoring='f1')\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Best parameters set found on the train set:\")\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:12,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.808     0.886     0.845      4342\n",
      "           1      0.826     0.721     0.770      3271\n",
      "\n",
      "    accuracy                          0.815      7613\n",
      "   macro avg      0.817     0.803     0.808      7613\n",
      "weighted avg      0.816     0.815     0.813      7613\n",
      "\n",
      "F1: 0.770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "kf = StratifiedKFold(n_splits=5)\n",
    "y_trainCv, y_predCv =  list(), list()\n",
    "\n",
    "for train_index, test_index in tqdm(kf.split(np.array(X_train), y_train)):\n",
    "    X_tr, X_ts = np.array(X_train)[train_index], np.array(X_train)[test_index]\n",
    "    y_tr, y_ts = y_train[train_index], y_train[test_index]\n",
    "    y_trainCv.extend(y_ts)\n",
    "    \n",
    "    clf = LogisticRegression(C=1, penalty='l2', solver='liblinear')\n",
    "#    clf = SVC()\n",
    "    clf.fit(X_tr, y_tr)\n",
    "    predictions = clf.predict(X_ts)\n",
    "    y_predCv.extend(predictions)\n",
    "\n",
    "print(classification_report(y_trainCv, y_predCv, digits=3, zero_division=False))\n",
    "print('F1: {:.3f}'.format(f1_score(y_trainCv, y_predCv, zero_division=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "# direct encoding\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "encoded_seq = tokenizer.encode(\"roberta is encoded\")\n",
    "\n",
    "# feature extraction\n",
    "feature_extraction = pipeline('feature-extraction', model=\"roberta-base\", tokenizer=\"roberta-base\", device=-1)\n",
    "features = feature_extraction(\"roberta is encoded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features[0][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
